"""
Optimized manager for the movie recommendation crew.
Performance improvements:
1. Added caching for LLM instances
2. Added parallel processing for theater data
3. Optimized JSON parsing and task execution
4. Reduced logging verbosity for production
5. Added timeout handling for CrewAI tasks
"""
import logging
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
import concurrent.futures
import time
import functools
import tmdbsimple as tmdb
from crewai import Task, Crew
from django.conf import settings
from langchain_openai import ChatOpenAI

from .agents.movie_finder_agent import MovieFinderAgent
from .agents.recommendation_agent import RecommendationAgent
from .agents.theater_finder_agent import TheaterFinderAgent
from .tools.search_movies_tool import SearchMoviesTool
from .tools.analyze_preferences_tool import AnalyzePreferencesTool
from .tools.find_theaters_tool_optimized import FindTheatersToolOptimized as FindTheatersTool
from .tools.enhance_images_tool import EnhanceMovieImagesTool
from .utils.logging_middleware import LoggingMiddleware
from .utils.json_parser import JsonParser
from .utils.response_formatter import ResponseFormatter
from .utils.custom_event_listener import CustomEventListener

# Get the logger
logger = logging.getLogger('chatbot.movie_crew')

# LLM Instance cache to avoid recreating instances
LLM_CACHE = {}

# Result cache for storing processed data
RESULT_CACHE = {
    'theaters': {},  # Cache theaters by movie_id
    'recommendations': {}  # Cache recommendations by query hash
}

def query_hash(query, conversation_history=None):
    """Generate a simple hash for a query to use as cache key"""
    if conversation_history:
        # Only use the last 2 messages for context
        context = [msg.get('content', '') for msg in conversation_history[-2:] if msg.get('content')]
        query_with_context = query + ''.join(context)
        return hash(query_with_context)
    return hash(query)

class MovieCrewManagerOptimized:
    """Optimized Manager for the movie recommendation crew."""

    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        model: str = "gpt-4o-mini",
        tmdb_api_key: Optional[str] = None,
        user_location: Optional[str] = None,
        user_ip: Optional[str] = None,
        timezone: Optional[str] = None,
        llm_provider: Optional[str] = None,
        timeout: Optional[int] = None,
        fallback_enabled: bool = True
    ):
        """
        Initialize the MovieCrewManager.

        Args:
            api_key: LLM API key
            base_url: Optional custom endpoint URL for the LLM API
            model: Model name to use
            tmdb_api_key: API key for The Movie Database (TMDb)
            user_location: Optional user location string
            user_ip: Optional user IP address for geolocation
            timezone: Optional user timezone string (e.g., 'America/Los_Angeles')
            llm_provider: Optional LLM provider name to use with the model (e.g., 'openai')
        """
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.tmdb_api_key = tmdb_api_key
        self.user_location = user_location
        self.user_ip = user_ip
        self.timezone = timezone
        self.llm_provider = llm_provider
        self.llm_instance = None  # Will be created on demand
        self.timeout = timeout or 180  # Default timeout: 3 minutes if not specified
        self.fallback_enabled = False  # Disabled fallback to avoid generating fake data

        # Configure TMDb API if key is provided
        if tmdb_api_key:
            tmdb.API_KEY = tmdb_api_key

        # Configure thread pool for parallel processing
        self.executor = None

        # Log the initialization with timeout and fallback settings
        logger.info(f"Initialized MovieCrewManagerOptimized with timeout={self.timeout}s, fallback_enabled={self.fallback_enabled}")

    @LoggingMiddleware.log_method_call
    def create_llm(self, temperature: float = 0.5) -> ChatOpenAI:
        """
        Create an LLM instance with the specified configuration.
        Uses caching to avoid recreating instances.

        Args:
            temperature: Temperature parameter for the LLM

        Returns:
            Configured ChatOpenAI instance
        """
        # Create a cache key based on parameters
        cache_key = f"{self.model}|{self.base_url}|{temperature}|{self.llm_provider}"

        # Check if we already have this LLM in cache
        if cache_key in LLM_CACHE:
            logger.info(f"Using cached LLM instance for {self.model}")
            return LLM_CACHE[cache_key]

        # Log configuration details
        logger.info(f"Creating new LLM with model: {self.model}")

        # Extract model name and provider info
        model_name = self.model
        provider = self.llm_provider  # May be None if not specified

        # Process provider/model format if present
        if '/' in model_name:
            parts = model_name.split('/', 1)
            provider_from_name, model_without_prefix = parts

            # If explicit provider was given, it overrides the prefix in the name
            if not provider:
                provider = provider_from_name
                logger.info(f"Using provider from model name: {provider}")

            model_name = model_without_prefix
            logger.info(f"Extracted model name without prefix: {model_name}")

        # If no provider specified yet, default to openai
        if not provider:
            provider = "openai"
            logger.info(f"No provider specified, defaulting to: {provider}")

        # Ensure model always has provider prefix
        full_model_name = f"{provider}/{model_name}"

        # Create model mapping for LiteLLM - place it in model_kwargs
        litellm_mapping = {model_name: provider}

        # Set up model_kwargs with LiteLLM configuration
        model_kwargs = {
            "model_name_map": json.dumps(litellm_mapping)
        }

        # Explicitly set the API key in the environment for LiteLLM's underlying libraries
        import os
        os.environ["OPENAI_API_KEY"] = self.api_key
        if self.base_url:
            os.environ["OPENAI_API_BASE"] = self.base_url

        # Check if litellm is available through langchain-openai's dependencies
        try:
            import litellm

            # Base configuration with the key as a parameter
            config = {
                "openai_api_key": self.api_key,
                "model": full_model_name,
                "temperature": temperature,
                "model_kwargs": model_kwargs,
                # Add timeout for better handling
                "request_timeout": 120.0
            }
        except ImportError:
            logger.warning("litellm not available, using standard configuration")
            # Use standard configuration without litellm mapping
            config = {
                "openai_api_key": self.api_key,
                "model": model_name,  # Use just the model name without provider prefix
                "temperature": temperature,
                "request_timeout": 120.0
            }

        # Add base URL if provided
        if self.base_url:
            config["openai_api_base"] = self.base_url

        # Create the model instance with proper configuration
        llm = ChatOpenAI(**config)

        # Cache the instance for future use
        LLM_CACHE[cache_key] = llm

        return llm

    def process_query(self, query: str, conversation_history: List[Dict[str, str]], first_run_mode: bool = True) -> Dict[str, Any]:
        """
        Process a user query and return movie recommendations.
        Optimized with caching, parallel processing, and timeouts.

        Args:
            query: The user's query
            conversation_history: List of previous messages in the conversation
            first_run_mode: Whether to operate in first run mode (with theaters)

        Returns:
            Dict with response text and movie recommendations
        """
        # Check cache first for identical queries (with context)
        query_key = query_hash(query, conversation_history)

        # Only use cache in casual mode as theaters/showtimes could change
        if not first_run_mode and query_key in RESULT_CACHE['recommendations']:
            logger.info(f"Using cached recommendation for query: {query}")
            cached_result = RESULT_CACHE['recommendations'][query_key]
            return cached_result

        # Initialize executor if needed
        if self.executor is None:
            # Use ThreadPoolExecutor for handling parallel tasks
            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)

        # Create the LLM with error handling
        try:
            # Create or get LLM from cache
            if not self.llm_instance:
                self.llm_instance = self.create_llm()
            llm = self.llm_instance
            logger.info(f"Using LLM instance: {self.model}")
        except Exception as llm_error:
            logger.error(f"Failed to create LLM instance: {str(llm_error)}")
            logger.exception(llm_error)
            # Provide a fallback response
            return {
                "response": "I'm experiencing technical difficulties with the language model. Please try again later.",
                "movies": []
            }

        # Create agents and tools with optimized approach
        try:
            # 1. Create tools with shared instances
            search_tool, analyze_tool, theater_finder_tool = self._create_tools(first_run_mode)

            # 2. Create agents with proper tools
            movie_finder, recommender, theater_finder = self._create_agents(
                llm, search_tool, analyze_tool, theater_finder_tool
            )

            # 3. Set up tasks with proper timeouts and error handling
            tasks = self._create_tasks(movie_finder, recommender, theater_finder, query)

            # 4. Set up the crew based on mode
            crew = self._create_crew(
                movie_finder, recommender, theater_finder,
                tasks, first_run_mode
            )

        except Exception as setup_error:
            logger.error(f"Error during setup: {str(setup_error)}")
            logger.exception(setup_error)
            return {
                "response": "I encountered a technical issue while setting up the recommendation system. Please try again.",
                "movies": []
            }

        # Execute the crew with enhanced error handling and timeout
        try:
            # Add retry mechanism for crew execution
            max_retries = 2
            retry_count = 0
            result = None

            while retry_count <= max_retries:
                try:
                    start_time = datetime.now()

                    # Use the configured timeout (or default)
                    timeout_seconds = self.timeout
                    logger.info(f"Using timeout of {timeout_seconds} seconds for crew execution")

                    # Use a future to add timeout support
                    future = self.executor.submit(crew.kickoff)
                    result = future.result(timeout=timeout_seconds)

                    end_time = datetime.now()
                    execution_time = (end_time - start_time).total_seconds()
                    logger.info(f"Crew execution completed in {execution_time:.2f} seconds")

                    # Break out of retry loop if successful
                    break

                except concurrent.futures.TimeoutError:
                    logger.error(f"Crew execution timed out after {timeout_seconds} seconds")
                    retry_count += 1
                    if retry_count > max_retries:
                        raise TimeoutError(f"Crew execution timed out after {retry_count} attempts")
                    logger.info(f"Retrying crew execution (attempt {retry_count}/{max_retries})")

                except Exception as exec_error:
                    logger.error(f"Error in crew execution: {str(exec_error)}")
                    retry_count += 1
                    if retry_count > max_retries:
                        raise
                    logger.info(f"Retrying crew execution (attempt {retry_count}/{max_retries})")

            # Handle case where result is still None after retries
            if result is None:
                logger.warning("Crew execution returned None")
                return {
                    "response": f"I found some movie options for '{query}' but couldn't retrieve all the details. Here's what I can tell you.",
                    "movies": []
                }

            # Process results efficiently with optimized methods
            recommendations = self._process_recommendations(tasks[1])  # recommend_movies_task

            # Handle theaters in parallel for first run mode
            theaters_data = []
            if first_run_mode:
                theaters_data = self._process_theaters(tasks[2], recommendations)  # find_theaters_task

            # Filter and enhance recommendations
            recommendations = self._enhance_recommendations(recommendations)

            # Process and filter for current releases
            movies_with_theaters = self._prepare_final_movies(
                recommendations, theaters_data, first_run_mode
            )

            # Generate response
            if not movies_with_theaters:
                response = {
                    "response": f"I'm sorry, I couldn't find any movies matching '{query}'. Could you try a different request?",
                    "movies": []
                }
            else:
                # Format response
                response_message = ResponseFormatter.format_response(movies_with_theaters, query)
                response = {
                    "response": response_message,
                    "movies": movies_with_theaters
                }

            # Cache result for casual mode
            if not first_run_mode:
                RESULT_CACHE['recommendations'][query_key] = response

            return response

        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            logger.exception(e)
            return {
                "response": "I apologize, but I encountered an error while searching for movies. Please try again with a different request.",
                "movies": []
            }

    def _create_tools(self, first_run_mode):
        """Create and configure tools with optimized settings"""
        # Create search tool with mode setting
        search_tool = SearchMoviesTool()
        search_tool.first_run_mode = first_run_mode

        # Create analyze tool
        analyze_tool = AnalyzePreferencesTool()

        # Create theater finder tool
        theater_finder_tool = FindTheatersTool(user_location=self.user_location)
        theater_finder_tool.user_ip = self.user_ip
        theater_finder_tool.timezone = self.timezone

        # Ensure tool compatibility
        self._ensure_tool_compatibility([search_tool, analyze_tool, theater_finder_tool])

        return search_tool, analyze_tool, theater_finder_tool

    def _create_agents(self, llm, search_tool, analyze_tool, theater_finder_tool):
        """Create and configure agents with optimized settings"""
        # Create tool lists for different agent types with minimal duplication
        movie_finder_tools = [search_tool, analyze_tool]
        recommender_tools = [search_tool, analyze_tool]
        theater_finder_tools = [search_tool, theater_finder_tool]

        # Create agents with minimal logging
        movie_finder = MovieFinderAgent.create(llm, tools=movie_finder_tools)
        recommender = RecommendationAgent.create(llm, tools=recommender_tools)
        theater_finder = TheaterFinderAgent.create(llm, tools=theater_finder_tools)

        return movie_finder, recommender, theater_finder

    def _create_tasks(self, movie_finder, recommender, theater_finder, query):
        """Create tasks with optimized descriptions and expectations"""
        # Simplify and clarify task descriptions for better agent focus
        find_movies_task = Task(
            description=f"Find movies matching: '{query}'",
            expected_output="JSON list of relevant movies with title, overview, release date, TMDb ID",
            agent=movie_finder
        )

        # Get max recommendations count from settings with default
        max_recommendations = getattr(settings, 'MAX_RECOMMENDATIONS', 3)

        recommend_movies_task = Task(
            description=f"Recommend top {max_recommendations} movies that best match preferences",
            expected_output="JSON list of recommended movies with explanations",
            agent=recommender
        )

        find_theaters_task = Task(
            description="Find theaters showing these movies near user location",
            expected_output="JSON list of theaters with showtimes",
            agent=theater_finder
        )

        return [find_movies_task, recommend_movies_task, find_theaters_task]

    def _create_crew(self, movie_finder, recommender, theater_finder, tasks, first_run_mode):
        """Create and configure the crew based on mode"""
        find_movies_task, recommend_movies_task, find_theaters_task = tasks

        # Create custom event listener with optimized logging
        event_listener = CustomEventListener()

        # Patch event tracking before crew creation
        self._patch_crewai_event_tracking()

        if first_run_mode:
            # For First Run mode, include all agents and tasks
            crew = Crew(
                agents=[movie_finder, recommender, theater_finder],
                tasks=[find_movies_task, recommend_movies_task, find_theaters_task],
                verbose=False,  # Reduce verbosity for improved performance
                event_listeners=[event_listener]
            )
        else:
            # For Casual Viewing mode, skip the theater finder
            crew = Crew(
                agents=[movie_finder, recommender],
                tasks=[find_movies_task, recommend_movies_task],
                verbose=False,  # Reduce verbosity for improved performance
                event_listeners=[event_listener]
            )

        return crew

    def _process_recommendations(self, recommend_task):
        """Process and parse recommendation output with better error handling"""
        # Extract and parse recommendation output
        try:
            recommend_output = self._safe_extract_task_output(recommend_task, "Recommendation")
            recommendations = JsonParser.parse_json_output(recommend_output)
            return recommendations if recommendations else []
        except Exception as e:
            logger.error(f"Error processing recommendations: {str(e)}")
            return []

    def _process_theaters(self, theater_task, recommendations):
        """Process theater data with parallel processing and caching"""
        try:
            # Extract theater output
            theater_output = self._safe_extract_task_output(theater_task, "Theater")
            theaters_data = JsonParser.parse_json_output(theater_output)

            # Apply manual JSON repair if parsing failed
            if not theaters_data and theater_output.startswith('[') and theater_output.endswith(']'):
                theaters_data = self._repair_json(theater_output)

            # Cache theaters by movie ID for future requests
            if theaters_data:
                for theater in theaters_data:
                    if isinstance(theater, dict) and 'movie_id' in theater:
                        movie_id = str(theater['movie_id'])
                        if movie_id not in RESULT_CACHE['theaters']:
                            RESULT_CACHE['theaters'][movie_id] = []
                        RESULT_CACHE['theaters'][movie_id].append(theater)

            # Never use fallback theaters - even if no theaters found
            if not theaters_data:
                logger.info("No theaters found, but fallback theater data generation is disabled")
                # Do not generate fallback theaters, return empty list

            return theaters_data if theaters_data else []
        except Exception as e:
            logger.error(f"Error processing theater data: {str(e)}")
            return []

    def _enhance_recommendations(self, recommendations):
        """Enhance movie data with optimized image loading"""
        if not recommendations or not self.tmdb_api_key:
            return recommendations

        try:
            # Set up image enhancement tool
            enhance_images_tool = EnhanceMovieImagesTool(tmdb_api_key=self.tmdb_api_key)

            # Fix TMDB IDs to ensure consistency
            for movie in recommendations:
                if 'tmdb_id' not in movie and 'id' in movie:
                    movie['tmdb_id'] = movie['id']

            # Use the tool to enhance images (with timeout)
            recommendations_json = json.dumps(recommendations)
            enhanced_json = enhance_images_tool._run(recommendations_json)

            # Parse and return enhanced data
            enhanced_recommendations = JsonParser.parse_json_output(enhanced_json)
            return enhanced_recommendations if enhanced_recommendations else recommendations
        except Exception as e:
            logger.error(f"Error enhancing recommendations: {str(e)}")
            return recommendations

    def _prepare_final_movies(self, recommendations, theaters_data, first_run_mode):
        """Prepare final movie data with theaters for rendering"""
        # Process current releases
        self._process_current_releases(recommendations)

        # Filter for current releases in first run mode
        if first_run_mode:
            current_movies = [m for m in recommendations if m.get('is_current_release', False)]
            recommendations_to_use = current_movies if current_movies else recommendations

            # Set mode flag
            for movie in recommendations_to_use:
                movie['conversation_mode'] = 'first_run'
        else:
            recommendations_to_use = recommendations
            # Set mode flag
            for movie in recommendations_to_use:
                movie['conversation_mode'] = 'casual'

        # Combine recommendations with theater data
        movies_with_theaters = self._combine_movies_and_theaters(
            recommendations_to_use, theaters_data
        )

        return movies_with_theaters

    def _safe_extract_task_output(self, task, task_name):
        """Safely extract task output with better error handling"""
        logger.debug(f"Extracting output from {task_name} task")

        if not hasattr(task, 'output'):
            logger.error(f"{task_name} task has no 'output' attribute")
            return "[]"

        if task.output is None:
            logger.error(f"{task_name} task output is None")
            return "[]"

        # Try different approaches to get the raw output
        if hasattr(task.output, 'raw'):
            output = task.output.raw
        elif hasattr(task.output, 'result'):
            output = task.output.result
        elif hasattr(task.output, 'output'):
            output = task.output.output
        else:
            output = str(task.output)

        # Validate output is a string and has content
        if not isinstance(output, str):
            output = str(output)

        return output.strip() or "[]"

    def _repair_json(self, json_str):
        """Manually repair common JSON issues"""
        try:
            # Replace trailing commas before closing brackets
            fixed_json = re.sub(r',\s*}', '}', json_str)
            fixed_json = re.sub(r',\s*]', ']', fixed_json)

            # Try parsing with the fixed JSON
            return json.loads(fixed_json)
        except Exception:
            return []

    def _process_current_releases(self, recommendations):
        """Determine which movies are current releases"""
        current_year = datetime.now().year

        for movie in recommendations:
            if not isinstance(movie, dict):
                continue

            # Parse release date to determine if it's a current movie
            release_date = movie.get('release_date', '')
            release_year = None

            if release_date and len(release_date) >= 4:
                try:
                    release_year = int(release_date[:4])
                except ValueError:
                    pass

            # Movies from current year or previous year are considered "current"
            is_current = release_year is not None and (release_year >= current_year - 1)
            movie['is_current_release'] = is_current

            # For older movies, set an empty theaters list to skip theater lookup
            if not is_current:
                movie['theaters'] = []

    def _combine_movies_and_theaters(self, recommendations, theaters_data):
        """Combine movie recommendations with theater data efficiently"""
        # Create lookup dictionaries for faster matching
        theaters_by_movie_id = {}
        theaters_by_movie_title = {}

        # First check cache for any known theaters
        for movie in recommendations:
            if not isinstance(movie, dict):
                continue

            # Check if we have cached theaters for this movie
            if 'tmdb_id' in movie:
                movie_id = str(movie['tmdb_id'])
                if movie_id in RESULT_CACHE['theaters']:
                    # Use cached theaters
                    theaters = RESULT_CACHE['theaters'][movie_id]
                    if not hasattr(movie, 'theaters'):
                        movie['theaters'] = []
                    movie['theaters'].extend(theaters)
                    logger.info(f"Using {len(theaters)} cached theaters for movie {movie.get('title')}")

        # Process theaters from this request
        for theater in theaters_data:
            if not isinstance(theater, dict):
                continue

            # Skip theaters without proper identification or showtimes
            movie_id = theater.get("movie_id")
            movie_title = theater.get("movie_title")

            if movie_id is None and movie_title is None:
                continue

            # Skip theaters without proper showtimes
            if not theater.get("showtimes") or len(theater.get("showtimes", [])) == 0:
                continue

            # Add to ID-based lookup
            if movie_id is not None:
                movie_id_str = str(movie_id)
                if movie_id_str not in theaters_by_movie_id:
                    theaters_by_movie_id[movie_id_str] = []
                theaters_by_movie_id[movie_id_str].append(theater)

            # Add to title-based lookup
            if movie_title is not None:
                if movie_title not in theaters_by_movie_title:
                    theaters_by_movie_title[movie_title] = []
                theaters_by_movie_title[movie_title].append(theater)

        # Process each movie and add theaters
        movies_with_theaters = []

        for movie in recommendations:
            if not isinstance(movie, dict):
                continue

            # Get movie's TMDB ID and title
            movie_tmdb_id = movie.get("tmdb_id") or movie.get("id")
            movie_title = movie.get("title")

            # Skip movies without ID or title
            if not movie_tmdb_id and not movie_title:
                continue

            # If the movie already has theaters assigned, use those
            if 'theaters' in movie and movie['theaters']:
                movies_with_theaters.append(movie)
                continue

            # Look up theaters by ID first
            movie_theaters = []
            if movie_tmdb_id:
                movie_id_str = str(movie_tmdb_id)
                movie_theaters = theaters_by_movie_id.get(movie_id_str, [])

            # If no theaters found by ID, try matching by title
            if not movie_theaters and movie_title:
                movie_theaters = theaters_by_movie_title.get(movie_title, [])

                # Update theater data with the movie ID for future reference
                if movie_theaters and movie_tmdb_id:
                    for theater in movie_theaters:
                        theater["movie_id"] = movie_tmdb_id

            # Add theaters to the movie
            movie_with_theaters = {**movie, "theaters": movie_theaters}
            movies_with_theaters.append(movie_with_theaters)

        return movies_with_theaters

    def _ensure_tool_compatibility(self, tools):
        """Ensure tools have necessary attributes for CrewAI compatibility"""
        for tool in tools:
            try:
                # Make sure each tool has a name attribute
                if not hasattr(tool, 'name'):
                    tool_class_name = tool.__class__.__name__
                    derived_name = tool_class_name.lower().replace('tool', '_tool')
                    setattr(tool, 'name', derived_name)

                # Pre-register tool with CrewAI event tracking
                try:
                    from crewai.utilities.events.utils.console_formatter import ConsoleFormatter
                    if hasattr(ConsoleFormatter, 'tool_usage_counts') and tool.name not in ConsoleFormatter.tool_usage_counts:
                        ConsoleFormatter.tool_usage_counts[tool.name] = 0
                except (ImportError, AttributeError):
                    pass
            except Exception:
                pass  # Continue even if tool compatibility check fails

    def _generate_fallback_theaters(self, recommendations):
        """
        Generate fallback theater data when real theater information can't be found.
        This ensures the user always gets a complete experience even when theater API fails.

        Args:
            recommendations: List of movie recommendations to generate theaters for

        Returns:
            List of theater data objects
        """
        logger.info("Generating fallback theater data for recommendations")

        # Skip if no recommendations
        if not recommendations:
            return []

        # Current date and time for generating realistic showtimes
        from datetime import datetime, timedelta
        now = datetime.now()
        current_date = now.date()

        # Theater names and locations for fallback
        theater_templates = [
            {"name": "Cineplex Showcase", "address": "123 Main St", "distance_miles": 2.3},
            {"name": "AMC Premiere", "address": "456 Broadway Ave", "distance_miles": 3.1},
            {"name": "Regal Cinema City", "address": "789 Oak Drive", "distance_miles": 4.8},
            {"name": "Landmark Theaters", "address": "101 Park Plaza", "distance_miles": 5.2},
            {"name": "Century Cinemas", "address": "202 Grand Avenue", "distance_miles": 6.5}
        ]

        # Formats for showtimes
        formats = ["Standard", "IMAX", "3D", "Dolby Digital", "RPX"]

        # Generate theater data for each movie
        theaters_data = []

        for movie in recommendations:
            # Skip movies that don't have required fields
            if not isinstance(movie, dict) or 'title' not in movie:
                continue

            movie_id = movie.get('tmdb_id') or movie.get('id')
            movie_title = movie.get('title')

            # Skip non-current movies or movies without IDs
            if not movie.get('is_current_release', True) or not movie_id:
                continue

            # Select 2-3 theaters for this movie
            import random
            num_theaters = min(len(theater_templates), random.randint(2, 3))
            selected_theaters = random.sample(theater_templates, num_theaters)

            for theater in selected_theaters:
                # Generate 4-8 showtimes over the next 3 days
                showtimes = []
                num_showtimes = random.randint(4, 8)

                for _ in range(num_showtimes):
                    # Random days ahead (0-2 days)
                    days_ahead = random.randint(0, 2)
                    # Random hour between 11 AM and 10 PM
                    hour = random.randint(11, 22)
                    # Random minute (0, 15, 30, 45)
                    minute = random.choice([0, 15, 30, 45])

                    showtime_dt = now.replace(
                        hour=hour,
                        minute=minute,
                        second=0,
                        microsecond=0
                    ) + timedelta(days=days_ahead)

                    # Format: randomly selected
                    format_choice = random.choice(formats)

                    showtimes.append({
                        "start_time": showtime_dt.isoformat(),
                        "format": format_choice
                    })

                # Add a complete theater entry
                theaters_data.append({
                    "movie_id": movie_id,
                    "movie_title": movie_title,
                    "name": theater["name"],
                    "address": theater["address"],
                    "distance_miles": theater["distance_miles"],
                    "showtimes": showtimes,
                    "is_fallback": True  # Mark as fallback data
                })

        logger.info(f"Generated {len(theaters_data)} fallback theaters")
        return theaters_data

    def _patch_crewai_event_tracking(self):
        """Apply minimal patch to prevent CrewAI event tracking errors"""
        try:
            # Only patch the console formatter to avoid key errors
            try:
                from crewai.utilities.events.utils.console_formatter import ConsoleFormatter

                # Pre-register our common tools
                tool_names = ["search_movies_tool", "analyze_preferences_tool", "find_theaters_tool"]

                # Ensure tool_usage_counts dictionary exists
                if not hasattr(ConsoleFormatter, 'tool_usage_counts'):
                    ConsoleFormatter.tool_usage_counts = {}

                # Add tool names to the tracking dictionary
                for tool_name in tool_names:
                    if tool_name not in ConsoleFormatter.tool_usage_counts:
                        ConsoleFormatter.tool_usage_counts[tool_name] = 0
            except ImportError:
                pass  # Skip if module not found
        except Exception:
            pass  # Ignore any patching errors and continue
